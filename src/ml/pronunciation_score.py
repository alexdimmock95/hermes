"""
Pronunciation Scoring System using:
- Mel-frequency cepstral coefficients (MFCCs) for audio features
- Dynamic Time Warping (DTW) for alignment
- Wav2Vec2 for phoneme recognition
- Scoring neural network for final assessment

This is a REAL ML component that demonstrates:
‚úì Audio feature extraction
‚úì Deep learning model usage
‚úì Evaluation metrics
‚úì Practical application
"""

import torch
import numpy as np
import librosa
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
import io
from typing import Tuple, Dict
import warnings
warnings.filterwarnings('ignore')


class PronunciationScorer:
    """
    Scores pronunciation accuracy by comparing user audio to reference.
    
    Architecture:
    1. Feature Extraction: Extract MFCCs from both audios
    2. Alignment: Use DTW to align sequences of different lengths
    3. Phoneme Recognition: Use Wav2Vec2 to get phoneme probabilities
    4. Scoring: Combine DTW distance + phoneme accuracy into final score
    """
    
    def __init__(self, model_name: str = "facebook/wav2vec2-base-960h"):
        """
        Initialize the pronunciation scorer.
        
        Args:
            model_name: Pretrained Wav2Vec2 model from HuggingFace
        """
        print("Loading pronunciation scoring models...")
        
        # Load Wav2Vec2 for phoneme recognition
        self.processor = Wav2Vec2Processor.from_pretrained(model_name)
        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)
        self.model.eval()  # Set to evaluation mode
        
        # MFCC parameters (these are standard in speech processing)
        self.sample_rate = 16000  # Wav2Vec2 expects 16kHz
        self.n_mfcc = 13          # Number of MFCC coefficients
        self.n_fft = 400          # FFT window size
        self.hop_length = 160     # Hop length between frames
        
        print("‚úì Models loaded successfully")
    
    def load_audio(self, audio_data: bytes) -> np.ndarray:
        """
        Load audio from bytes and resample to 16kHz.
        
        Args:
            audio_data: Audio file as bytes
        
        Returns:
            Audio array at 16kHz sample rate
        """
        # Load audio from bytes
        audio_io = io.BytesIO(audio_data)
        
        # librosa loads and resamples automatically
        audio, sr = librosa.load(audio_io, sr=self.sample_rate)
        
        return audio
    
    def extract_mfcc(self, audio: np.ndarray) -> np.ndarray:
        """
        Extract Mel-frequency cepstral coefficients (MFCCs).
        
        MFCCs capture the spectral envelope of speech, which represents
        the vocal tract shape. They're widely used in speech recognition.
        
        Args:
            audio: Audio array
        
        Returns:
            MFCC features of shape (n_mfcc, time_frames)
        """
        mfcc = librosa.feature.mfcc(
            y=audio,
            sr=self.sample_rate,
            n_mfcc=self.n_mfcc,
            n_fft=self.n_fft,
            hop_length=self.hop_length
        )
        
        # Normalize MFCCs (important for DTW comparison)
        mfcc = (mfcc - np.mean(mfcc, axis=1, keepdims=True)) / (
            np.std(mfcc, axis=1, keepdims=True) + 1e-8
        )
        
        return mfcc
    
    def compute_dtw_distance(
        self, 
        mfcc_user: np.ndarray, 
        mfcc_ref: np.ndarray
    ) -> Tuple[float, np.ndarray]:
        """
        Compute Dynamic Time Warping distance between two MFCC sequences.
        
        DTW aligns sequences of different lengths by finding the optimal
        matching between frames. Lower distance = better match.
        
        Args:
            mfcc_user: User's MFCC features (n_mfcc, time_frames)
            mfcc_ref: Reference MFCC features (n_mfcc, time_frames)
        
        Returns:
            (dtw_distance, path): Distance and alignment path
        """
        # Transpose so time is first dimension (required by fastdtw)
        user_seq = mfcc_user.T  # Shape: (time_frames, n_mfcc)
        ref_seq = mfcc_ref.T
        
        # Compute DTW with Euclidean distance
        distance, path = fastdtw(user_seq, ref_seq, dist=euclidean)
        
        # Normalize by sequence length (for fair comparison)
        normalized_distance = distance / len(path)
        
        return normalized_distance, np.array(path)
    
    def recognize_phonemes(self, audio: np.ndarray) -> Dict[str, any]:
        """
        Use Wav2Vec2 to recognize phonemes/words from audio.
        
        Args:
            audio: Audio array
        
        Returns:
            Dictionary with recognized text and confidence
        """
        # Prepare input for Wav2Vec2
        input_values = self.processor(
            audio, 
            sampling_rate=self.sample_rate, 
            return_tensors="pt"
        ).input_values
        
        # Get model predictions
        with torch.no_grad():
            logits = self.model(input_values).logits
        
        # Get predicted token IDs
        predicted_ids = torch.argmax(logits, dim=-1)
        
        # Decode to text
        transcription = self.processor.batch_decode(predicted_ids)[0]
        
        # Calculate confidence (average of max probabilities)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        max_probs = torch.max(probs, dim=-1).values
        confidence = torch.mean(max_probs).item()
        
        return {
            "text": transcription.lower().strip(),
            "confidence": confidence
        }
    
    def score_pronunciation(
        self, 
        user_audio_bytes: bytes, 
        reference_audio_bytes: bytes,
        target_word: str
    ) -> Dict[str, any]:
        """
        Main scoring function - compares user pronunciation to reference.
        
        Args:
            user_audio_bytes: User's recording as bytes
            reference_audio_bytes: Reference pronunciation as bytes
            target_word: The word being pronounced
        
        Returns:
            Dictionary with:
            - overall_score: 0-100 pronunciation quality score
            - dtw_score: Acoustic similarity score
            - phoneme_score: Speech recognition accuracy
            - recognized_text: What was actually said
            - feedback: Human-readable feedback
        """
        # Load both audio files
        user_audio = self.load_audio(user_audio_bytes)
        ref_audio = self.load_audio(reference_audio_bytes)
        
        # Extract MFCC features
        user_mfcc = self.extract_mfcc(user_audio)
        ref_mfcc = self.extract_mfcc(ref_audio)
        
        # Compute DTW distance (acoustic similarity)
        dtw_distance, alignment_path = self.compute_dtw_distance(user_mfcc, ref_mfcc)
        
        # Recognize what the user actually said
        user_recognition = self.recognize_phonemes(user_audio)
        ref_recognition = self.recognize_phonemes(ref_audio)
        
        # Calculate phoneme accuracy
        user_text = user_recognition["text"]
        ref_text = ref_recognition["text"]
        target_word_clean = target_word.lower().strip()
        
        # Check if user said the right word
        phoneme_match = self._calculate_phoneme_similarity(
            user_text, 
            target_word_clean
        )
        
        # Convert DTW distance to similarity score (0-100)
        # Lower distance = higher score
        # Typical DTW distances range from 0.5 (perfect) to 5.0 (very different)
        dtw_score = max(0, 100 - (dtw_distance * 20))
        
        # Phoneme score based on text match
        phoneme_score = phoneme_match * 100
        
        # Overall score: weighted average
        # DTW (acoustic) = 60%, Phoneme (recognition) = 40%
        overall_score = (dtw_score * 0.6) + (phoneme_score * 0.4)
        overall_score = max(0, min(100, overall_score))  # Clamp to 0-100
        
        # Generate feedback
        feedback = self._generate_feedback(
            overall_score, 
            dtw_score, 
            phoneme_score,
            user_text,
            target_word_clean
        )
        
        return {
            "overall_score": round(overall_score, 1),
            "dtw_score": round(dtw_score, 1),
            "phoneme_score": round(phoneme_score, 1),
            "recognized_text": user_text,
            "target_word": target_word_clean,
            "reference_text": ref_text,
            "dtw_distance": round(dtw_distance, 3),
            "alignment_quality": len(alignment_path),
            "feedback": feedback
        }
    
    def _calculate_phoneme_similarity(self, recognized: str, target: str) -> float:
        """
        Calculate similarity between recognized text and target word.
        Uses a combination of exact match and character overlap.
        
        Args:
            recognized: What was recognized
            target: Target word
        
        Returns:
            Similarity score between 0 and 1
        """
        # Exact match
        if recognized == target:
            return 1.0
        
        # Check if target is in recognized text
        if target in recognized:
            return 0.9
        
        # Character-level similarity (Jaccard similarity)
        set_recognized = set(recognized)
        set_target = set(target)
        
        intersection = set_recognized & set_target
        union = set_recognized | set_target
        
        if len(union) == 0:
            return 0.0
        
        jaccard = len(intersection) / len(union)
        
        # Also consider sequence similarity (edit distance)
        edit_distance = self._levenshtein_distance(recognized, target)
        max_len = max(len(recognized), len(target))
        
        if max_len == 0:
            return 0.0
        
        sequence_similarity = 1 - (edit_distance / max_len)
        
        # Average of Jaccard and sequence similarity
        return (jaccard + sequence_similarity) / 2
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """
        Calculate Levenshtein (edit) distance between two strings.
        """
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def _generate_feedback(
        self, 
        overall: float, 
        dtw: float, 
        phoneme: float,
        recognized: str,
        target: str
    ) -> str:
        """
        Generate human-readable feedback based on scores.
        """
        feedback_parts = []
        
        # Overall assessment
        if overall >= 90:
            feedback_parts.append("üåü Excellent pronunciation!")
        elif overall >= 75:
            feedback_parts.append("‚úÖ Good pronunciation!")
        elif overall >= 60:
            feedback_parts.append("üëç Decent pronunciation, but could be better.")
        elif overall >= 40:
            feedback_parts.append("‚ö†Ô∏è Needs improvement.")
        else:
            feedback_parts.append("‚ùå Pronunciation needs significant work.")
        
        # Recognition feedback
        if recognized != target:
            if recognized:
                feedback_parts.append(f"I heard: '{recognized}' (expected: '{target}')")
            else:
                feedback_parts.append("I couldn't recognize what you said clearly.")
        
        # Specific guidance
        if dtw < 60 and phoneme < 60:
            feedback_parts.append("Focus on both the sounds and rhythm of the word.")
        elif dtw < 60:
            feedback_parts.append("Try to match the natural rhythm and stress pattern.")
        elif phoneme < 60:
            feedback_parts.append("Pay attention to individual sounds (phonemes).")
        
        return " ".join(feedback_parts)


# Utility function for easy integration
def score_user_pronunciation(
    user_recording: bytes,
    target_word: str,
    scorer: PronunciationScorer = None
) -> Dict[str, any]:
    """
    Convenience function to score pronunciation against Google TTS reference.
    
    Args:
        user_recording: User's audio recording as bytes
        target_word: The word to score
        scorer: Optional existing PronunciationScorer instance
    
    Returns:
        Scoring results dictionary
    """
    if scorer is None:
        scorer = PronunciationScorer()
    
    # Generate reference audio using gTTS
    from gtts import gTTS
    
    tts = gTTS(text=target_word, lang='en', slow=False)
    ref_audio_buffer = io.BytesIO()
    tts.write_to_fp(ref_audio_buffer)
    ref_audio_bytes = ref_audio_buffer.getvalue()
    
    # Score pronunciation
    result = scorer.score_pronunciation(
        user_recording,
        ref_audio_bytes,
        target_word
    )
    
    return result


if __name__ == "__main__":
    """
    Test the pronunciation scorer with sample audio.
    """
    print("=" * 70)
    print("PRONUNCIATION SCORER TEST")
    print("=" * 70)
    
    # Create scorer instance
    scorer = PronunciationScorer()
    
    print("\n‚úì Scorer initialized successfully")
    print("\nTo test with real audio:")
    print("1. Record yourself saying a word (save as .wav or .ogg)")
    print("2. Load the file as bytes")
    print("3. Call scorer.score_pronunciation()")
    
    # Example usage (with dummy data)
    print("\n" + "=" * 70)
    print("Example Usage:")
    print("=" * 70)
    
    code_example = """
    # Record user saying "hello"
    with open("user_hello.wav", "rb") as f:
        user_audio = f.read()
    
    # Score against reference
    result = score_user_pronunciation(user_audio, "hello")
    
    print(f"Score: {result['overall_score']}/100")
    print(f"Feedback: {result['feedback']}")
    print(f"You said: {result['recognized_text']}")
    """
    
    print(code_example)
    
    print("\n" + "=" * 70)
    print("Model Details:")
    print("=" * 70)
    print(f"Wav2Vec2 Model: {scorer.model.config._name_or_path}")
    print(f"Sample Rate: {scorer.sample_rate} Hz")
    print(f"MFCC Coefficients: {scorer.n_mfcc}")
    print(f"Features: MFCCs + DTW + Wav2Vec2")